{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1542f8a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc0c2e0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting python_docx\n",
      "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lxml>=2.3.2\n",
      "  Downloading lxml-4.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: python_docx\n",
      "  Building wheel for python_docx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python_docx: filename=python_docx-0.8.11-py3-none-any.whl size=184490 sha256=061816c1880f685e9969ac1f17503b95097ee76072101c0fcba9acc6e84143a1\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/65/e1/9b/0c38fe6cfe02a9fe31cb6b4efd90985f17354d7f77872f2def\n",
      "Successfully built python_docx\n",
      "Installing collected packages: lxml, python_docx\n",
      "Successfully installed lxml-4.9.2 python_docx-0.8.11\n"
     ]
    }
   ],
   "source": [
    "!pip install python_docx\n",
    "!pip install --upgrade boto3\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3680aba6-c25e-4bec-9d64-e670c7dac856",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b473cddd-6315-45d2-91cc-c1954a09c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of index\n",
    "index_name = ' '\n",
    "\n",
    "# The name of embbeding model endpoint, usually you can keep it as default\n",
    "eb_endpoint = 'huggingface-inference-text2vec-base-chinese-v1'\n",
    "\n",
    "# Ebbeding vector dimension, usually you can keep it as default\n",
    "v_dimension = 768\n",
    "\n",
    "# Docs file folder to be processed and ingested\n",
    "folder_path = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92c23b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " import 11 out of 12 finished file 0 out of 1 finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import json\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "hfp = sagemaker.huggingface.model.HuggingFacePredictor('huggingface-inference-text2vec-base-chinese-v1')\n",
    "\n",
    "#===================Function Definition=================\n",
    "def is_all_black(s):\n",
    "    for si in s:\n",
    "        if si != ' ':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def read_doc(path):\n",
    "    title = get_title(path)\n",
    "    titles = []\n",
    "    paragraphs = []\n",
    "    sentences = []\n",
    "    paragraphs_id = []\n",
    "    sentences_id = []\n",
    "    \n",
    "    document = Document(path)  # 读入文件\n",
    "    for i in range(len(document.paragraphs)):\n",
    "        p0 = document.paragraphs[i].text\n",
    "        p = document.paragraphs[i].text.replace('. ', '。')\n",
    "        if p != '':\n",
    "            ss = p.split('。')\n",
    "            for j in range(len(ss)):\n",
    "                if ss[j] != '' and is_all_black(ss[j])==False:\n",
    "                    titles.append(title)\n",
    "                    paragraphs.append(p0)\n",
    "                    sentences.append(ss[j])\n",
    "                    paragraphs_id.append(i)\n",
    "                    sentences_id.append(j)\n",
    "    df = pd.DataFrame({'title':titles, 'paragraph':paragraphs, 'sentence':sentences,\n",
    "                      'paragraph_id':paragraphs_id, 'sentence_id':sentences_id})          \n",
    "    return df\n",
    "\n",
    "def get_title(path):\n",
    "    try:\n",
    "        title = os.path.split(os.path.splitext(path)[0])[1].replace('——', '-').split('-')[1]\n",
    "    except:\n",
    "        title = os.path.split(os.path.splitext(path)[0])[1]\n",
    "    return title\n",
    "\n",
    "def get_vector(q):\n",
    "    if len(q) > 400:\n",
    "        return [-1000 for _ in range(768)]\n",
    "    return hfp.predict({'inputs':[q]})[0][0][0]\n",
    "\n",
    "def embbeding(df):\n",
    "    df['title_vector'] = ''\n",
    "    df['sentence_vector'] = ''\n",
    "    title_vector = str(get_vector(df.iloc[0, 0]))\n",
    "    for i in range(len(df)):\n",
    "        df.iloc[i, 5] = title_vector\n",
    "        df.iloc[i, 6] = str(get_vector(df.iloc[i, 2]))\n",
    "        print('\\r embbeding %i out of %i finished'%(i, len(df)), end='')\n",
    "    return df\n",
    "\n",
    "# ==============OpenSearch Related=====================\n",
    "# retrieve secret manager value by key using boto3\n",
    "sm_client = boto3.client('secretsmanager')\n",
    "master_user = sm_client.get_secret_value(SecretId='opensearch-host-url')['SecretString']\n",
    "data= json.loads(master_user)\n",
    "es_host_name = data.get('host')\n",
    "host = es_host_name+'/' if es_host_name[-1] != '/' else es_host_name# cluster endpoint, for example: my-test-domain.us-east-1.es.amazonaws.com/\n",
    "region = boto3.Session().region_name # e.g. cn-north-1\n",
    "# sm_client = boto3.client('secretsmanager')\n",
    "master_user = sm_client.get_secret_value(SecretId='opensearch-master-user')['SecretString']\n",
    "data= json.loads(master_user)\n",
    "username = data.get('username')\n",
    "password = data.get('password')\n",
    "# service = 'es'\n",
    "# credentials = boto3.Session().get_credentials()\n",
    "awsauth = (username, password)\n",
    "url = host+'_bulk'\n",
    "headers = { \"Content-Type\": \"application/json\" }\n",
    "\n",
    "payloads = {\n",
    "\"settings\": { \"index\": {\n",
    "\"knn\": True,\n",
    "\"knn.algo_param.ef_search\": 100 }\n",
    "}, \"mappings\": {\n",
    "\"properties\": { \n",
    "  \"title_vector\": {\n",
    "\"type\": \"knn_vector\", \"dimension\": v_dimension, \"method\": {\n",
    "\"name\": \"hnsw\", \"space_type\": \"l2\", \"engine\": \"nmslib\", \"parameters\": {\n",
    "\"ef_construction\": 256,\n",
    "\"m\": 48 }\n",
    "} },\n",
    "\"sentence_vector\": {\n",
    "\"type\": \"knn_vector\", \"dimension\": v_dimension, \"method\": {\n",
    "\"name\": \"hnsw\", \"space_type\": \"l2\", \"engine\": \"nmslib\", \"parameters\": {\n",
    "\"ef_construction\": 256,\n",
    "\"m\": 48 }\n",
    "} },\n",
    "\"title\": { \"type\": \"text\"}, \n",
    "\"sentence\": {\"type\": \"text\" }, \n",
    "\"paragraph\": {\"type\": \"text\" }, \n",
    "\"sentence_id\": {\"type\": \"text\" }, \n",
    "\"paragraph_id\": {\"type\": \"text\" }\n",
    "} }\n",
    "}\n",
    "\n",
    "# Create Index\n",
    "r = requests.put(host+index_name, auth=awsauth, headers=headers, json=payloads)\n",
    "\n",
    "def import_data(df, id_start=0, before_import=0):\n",
    "    payloads = ''\n",
    "    for i in range(id_start, len(df)+id_start):\n",
    "        first = json.dumps({ \"index\": { \"_index\": index_name, \"_id\": str(i+before_import) } }, ensure_ascii=False) + \"\\n\"\n",
    "        second = json.dumps({\"title\": str(df.iloc[i-id_start, 0]), \n",
    "                     \"paragraph\": str(df.iloc[i-id_start, 1]), \n",
    "                     \"sentence\": str(df.iloc[i-id_start, 2]), \n",
    "                     \"paragraph_id\": str(df.iloc[i-id_start, 3]), \n",
    "                     \"sentence_id\": str(df.iloc[i-id_start, 4]), \n",
    "                     \"title_vector\": json.loads(df.iloc[i-id_start, 5]),\n",
    "                     \"sentence_vector\": json.loads(df.iloc[i-id_start, 6])},\n",
    "                   ensure_ascii=False) + \"\\n\"\n",
    "        payloads += first + second\n",
    "    # print(payloads)\n",
    "    r = requests.post(url, auth=awsauth, headers=headers, data=payloads.encode()) # requests.get, post, and delete have similar syntax\n",
    "#     print(r.text)\n",
    "\n",
    "#==============Main Preprocess Data and Import===============\n",
    "\n",
    "slice = 10\n",
    "names = os.listdir(folder_path)\n",
    "before_import = 0\n",
    "for j in range(len(names)):\n",
    "    name = names[j]\n",
    "    if os.path.splitext(name)[1] not in ['.doc','.docx']:continue\n",
    "    df = read_doc(os.path.join(folder_path, name))\n",
    "    df = embbeding(df)\n",
    "    for i in range(len(df)//slice+1):\n",
    "        import_data(df[slice*i:slice*(i+1)], slice*i, before_import)\n",
    "        print('\\r import %i out of %i finished'%(i, len(df)//slice+1), end='')\n",
    "    before_import += len(df)\n",
    "    print(' file %i out of %i finished'%(j, len(names)//slice+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c9c513-4101-4992-a14b-dcf1ec9dbd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc59b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
