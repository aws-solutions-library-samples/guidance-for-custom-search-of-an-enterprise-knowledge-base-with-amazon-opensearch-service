{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241f04f-873e-4e22-bb59-57ee7abbd596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy --target ./python\n",
    "!pip install --upgrade numexpr --target ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b44f39c2-1b3c-40b1-a1a0-9239ab65316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"./python\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "from model import *\n",
    "\n",
    "#根据时间情况修改index和language值\n",
    "index =  \"\"\n",
    "embedding_endpoint_name = \"cohere.embed-multilingual-v3\"\n",
    "\n",
    "embedding_type = 'bedrock' if embedding_endpoint_name.find('titan') or embedding_endpoint_name.find('cohere') else 'sagemaker'\n",
    "embeddings = init_embeddings_bedrock(embedding_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f25c55-868a-483b-ad3e-329ab2d57906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"./python\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import fitz\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import base64\n",
    "from opensearch_multimodel_dataload import add_multimodel_documents\n",
    "import re\n",
    "\n",
    "model_name = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "# model_name = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "llm = init_model_bedrock(model_name)\n",
    "text_max_length = 2000\n",
    "\n",
    "def is_json(myjson):\n",
    "    try:\n",
    "        json.loads(myjson)\n",
    "    except ValueError as e:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a document manager at an insurance company and your task is to extract useful information from document images.\n",
    "<instructions>\n",
    "1.Output the document in markdown format, and keep the rows and columns aligned for the table.\n",
    "2.To retain all headers and table content, they cannot be omitted\n",
    "3.No preface, just output the document content directly.\n",
    "</instructions>\n",
    "\"\"\"\n",
    "\n",
    "files_path = '../docs/xxxxx/'\n",
    "# os.mkdir('images/')\n",
    "files = os.listdir(files_path)\n",
    "for file in files:\n",
    "    file_path = files_path + file\n",
    "    print(file_path)\n",
    "    # fname = file.split('/')[-1].split('.')[0]\n",
    "    # print(fname)\n",
    "    # os.mkdir('images/'+fname)\n",
    "\n",
    "    doc = fitz.open(file_path)\n",
    "    previous_page_content = ''\n",
    "    \n",
    "    texts = []\n",
    "    metadatas = []\n",
    "    images = []\n",
    "    \n",
    "    for i in tqdm(range(doc.page_count)):\n",
    "\n",
    "        page = doc.load_page(i)\n",
    "        pix = page.get_pixmap(dpi=100)\n",
    "        \n",
    "        # img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        # img_name = fname + '-' + str(i) + '.jpg'\n",
    "        # img.save('images/'+fname + '/' + img_name, \"JPEG\")\n",
    "        \n",
    "        imgb64 = base64.b64encode(pix.tobytes()).decode(\"utf-8\")\n",
    "        model_kwargs = {'image': imgb64,'max_tokens':2048}\n",
    "        llm.model_kwargs = model_kwargs\n",
    "        new_prompt = prompt.format(context=previous_page_content)\n",
    "        response = llm(prompt=new_prompt)\n",
    "        response = response.strip()\n",
    "        previous_page_content = response\n",
    "        \n",
    "        print('response:',response)\n",
    "\n",
    "        response_set = set()\n",
    "        response_list = response.split('\\n')\n",
    "        \n",
    "        header = []\n",
    "        for sentence in response_list:\n",
    "            sentence = sentence.strip()\n",
    "            data_list = re.split(r\"\\||<br>|/\", sentence)\n",
    "            for data in data_list:\n",
    "                data = sentence.strip()\n",
    "                if len(data) > 0:\n",
    "                    response_set.add(data)\n",
    "            \n",
    "            if sentence.find('|') >=0:\n",
    "                if len(header) == 0:\n",
    "                    header = sentence.split('|')\n",
    "                else:\n",
    "                    content = sentence.replace('-','').replace(':','').replace('|','')\n",
    "                    if len(content) > 0:\n",
    "                        content = sentence.split('|')\n",
    "                        line_str = ''\n",
    "                        for i in range(len(header)):\n",
    "                            if i < len(content) and len(str(header[i]).strip()) > 0 and len(str(content[i]).strip()) > 0:\n",
    "                                line_str += (str(header[i]).strip() + ':' + str(content[i]).strip()+ ',')\n",
    "                            elif i < len(content) and len(str(content[i]).strip()) > 0:\n",
    "                                line_str += (str(content[i]).strip()+ ',')\n",
    "                            else:\n",
    "                                line_str += (str(header[i]).strip()+ ',')\n",
    "                        response_set.add(line_str[:-1])\n",
    "\n",
    "            elif sentence.find('|') < 0 and len(header) > 0:\n",
    "                header = []\n",
    "                \n",
    "        for text in response_set:\n",
    "            text = text.strip()\n",
    "            print('text:',text)\n",
    "            print('--------------')\n",
    "            texts.append(str(response))\n",
    "            images.append(imgb64)\n",
    "            metadata = {}\n",
    "            metadata['sentence'] = text[:text_max_length] if len(text) > text_max_length else text\n",
    "            metadata['sources'] = file.split('/')[-1]\n",
    "            metadata['page'] = i\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "        if len(texts) > 0:\n",
    "            if embedding_type == 'bedrock':\n",
    "                text_embeddings = embeddings.embed_documents([metadata['sentence'] for metadata in metadatas])\n",
    "            else:\n",
    "                text_embeddings = embeddings.embed_documents([metadata['sentence'] for metadata in metadatas],chunk_size=10)\n",
    "\n",
    "            print('texts len:',len(texts))\n",
    "            print('metadatas len:',len(metadatas))\n",
    "            print('embeddings len:',len(text_embeddings))\n",
    "            print('images len:',len(images))\n",
    "            print('begin to save in vectore store')\n",
    "\n",
    "            add_multimodel_documents(\n",
    "                index,\n",
    "                texts=texts,\n",
    "                embeddings=text_embeddings,\n",
    "                metadatas=metadatas,\n",
    "                images=images\n",
    "            )\n",
    "        print('finish save in vectore store:',index)\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "        images = []\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
